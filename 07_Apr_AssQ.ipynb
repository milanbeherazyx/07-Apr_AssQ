{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Polynomial functions and kernel functions are related in machine learning algorithms in the sense that kernel functions can be used to implicitly represent polynomial functions in a high-dimensional feature space without actually computing the explicit polynomial features.\n",
    "\n",
    ">In other words, kernel functions allow us to transform the data into a higher-dimensional space where it may be more separable, without actually computing the feature mapping explicitly. Polynomial kernel functions are a specific type of kernel function that represent polynomial functions in this higher-dimensional feature space.\n",
    "\n",
    ">Polynomial kernel functions have the form K(x, x') = (x^T x' + c)^d, where x and x' are input vectors, d is the degree of the polynomial, and c is an optional constant term. This kernel function allows us to represent a polynomial function of degree d in the original input space without explicitly computing the polynomial features.\n",
    "\n",
    ">In summary, kernel functions, and polynomial kernel functions in particular, provide a powerful tool for nonlinear classification and regression problems by implicitly representing complex nonlinear functions in a high-dimensional feature space without the need to compute explicit polynomial features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate some data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM with polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set and calculate accuracy\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In Support Vector Regression (SVR), epsilon is a hyperparameter that determines the width of the margin that controls the trade-off between model complexity and error. Increasing the value of epsilon allows more data points to be within the margin or even on the wrong side of the hyperplane without being penalized. This means that the model becomes less sensitive to noise and outliers in the data, but at the cost of increased bias.\n",
    "\n",
    ">In terms of the number of support vectors, increasing epsilon generally leads to a decrease in the number of support vectors, as more data points are allowed to be outside the margin. Conversely, decreasing epsilon will lead to a larger margin, which may lead to more support vectors being included in the model. However, the relationship between epsilon and the number of support vectors is not always straightforward, as it also depends on the complexity of the data and the choice of kernel function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The performance of Support Vector Regression (SVR) is influenced by several parameters, including the choice of kernel function, C parameter, epsilon parameter, and gamma parameter. Each parameter has a unique impact on the model's performance.\n",
    "\n",
    "- Kernel Function: The kernel function maps the input data into a high-dimensional feature space, allowing for the separation of non-linearly separable data. Popular kernel functions include linear, polynomial, and radial basis function (RBF) kernels. The choice of kernel function depends on the data and the complexity of the problem. For example, a linear kernel is often used for linearly separable data, while a non-linear kernel like RBF is preferred for non-linearly separable data.\n",
    "\n",
    "- C Parameter: The C parameter controls the trade-off between achieving a low training error and a low testing error. A small value of C produces a wider margin that may lead to underfitting, while a larger value of C produces a narrower margin that may lead to overfitting. A smaller value of C may be suitable when there is high noise in the data, while a larger value may be suitable when the data is cleaner and the model needs to be more precise.\n",
    "\n",
    "- Epsilon Parameter: The epsilon parameter determines the margin of tolerance where no penalty is given to errors. A smaller value of epsilon results in a stricter margin, while a larger value results in a more relaxed margin. A smaller value of epsilon may be suitable when the model needs to be more precise, while a larger value may be suitable when the model needs to be more robust to outliers.\n",
    "\n",
    "- Gamma Parameter: The gamma parameter defines the influence of a single training example. A small gamma value means that the influence of each training example is larger, resulting in a more complex model that is more prone to overfitting. A large gamma value means that the influence of each training example is smaller, resulting in a smoother and simpler model. A smaller gamma value may be suitable when there are fewer training examples, while a larger gamma value may be suitable when there are more training examples.\n",
    "\n",
    ">In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter should be based on the data and the complexity of the problem. The values of these parameters should be tuned to achieve the best performance on the testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "-  Import the necessary libraries and load the dataset\n",
    "-  Split the dataset into training and testing sets\n",
    "-  Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "-  Create an instance of the SVC classifier and train it on the training data\n",
    "-  Use the trained classifier to predict the labels of the testing data\n",
    "-  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, Fl-score)\n",
    "-  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "-  Train the tuned classifier on the entire dataset\n",
    "-  Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best hyperparameters: {'C': 0.1, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['iris_svc.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Define the hyperparameters to be tuned\n",
    "params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "grid = GridSearchCV(svc, params, cv=5)\n",
    "grid.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc = SVC(C=grid.best_params_['C'], kernel=grid.best_params_['kernel'], degree=grid.best_params_['degree'], gamma=grid.best_params_['gamma'])\n",
    "tuned_svc.fit(iris.data, iris.target)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc, 'iris_svc.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
